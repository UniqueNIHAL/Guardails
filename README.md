# NeMo Guardrails + Groq Moderated Chatbot

This project integrates **NVIDIA NeMo Guardrails** with the **Groq LLM API** to create a secure and compliant AI chatbot.

---

## ðŸš€ Features

- âŒ Blocks political, illegal, and toxic content  
- âœ‚ï¸ Enforces response length (`max_tokens`)  
- ðŸ”Œ Exposes a FastAPI `/chat` endpoint  
- ðŸ› ï¸ Modular, secure, and extensible architecture

---

## ðŸ“ Project Structure

```plaintext
config/
  â”œâ”€â”€ config.yml       # LLM model config & max_tokens
  â”œâ”€â”€ rails.co         # Guardrails and moderation rules
  â””â”€â”€ prompts.yml      # Prompt templates for moderation
main.py                # FastAPI app entry point
requirements.txt       # Python dependencies
.env                   # Environment variables (API key, etc.)
```

---

## âš¡ Quickstart

1. **Set up a virtual environment**  
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

2. **Install dependencies**  
   ```bash
   pip install -r requirements.txt
   ```

3. **Add your Groq API key to `.env`**  
   ```env
   GROQ_API_KEY=your-groq-api-key-here
   ```

4. **Start the FastAPI server**  
   ```bash
   uvicorn main:app --reload
   ```

5. **Test the chatbot**  
   ```bash
   curl -X POST -H "Content-Type: application/json" \
     -d "{\"message\": \"Tell me a fun fact about computers.\"}" \
     http://localhost:8000/chat
   ```

---

## ðŸ§  How It Works

- ðŸ”’ User messages are filtered using NeMo Guardrails before reaching the LLM  
- ðŸ”Ž LLM responses are moderated before being returned  
- âš ï¸ Unsafe messages are blocked with refusal prompts; safe messages receive normal responses

---

## ðŸ› ï¸ Customization

- Modify `config/prompts.yml` to update moderation prompts  
- Change `max_tokens` in `config/config.yml` to set response limits

---

## âš ï¸ Important

> **Keep your `.env` file secret. Never commit your API key.**

---


