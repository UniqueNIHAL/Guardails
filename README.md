# NeMo Guardrails + Groq Moderated Chatbot

This project integrates **NVIDIA NeMo Guardrails** with the **Groq LLM API** to create a secure and compliant AI chatbot.

---

## 🚀 Features

- ❌ Blocks political, illegal, and toxic content  
- ✂️ Enforces response length (`max_tokens`)  
- 🔌 Exposes a FastAPI `/chat` endpoint  
- 🛠️ Modular, secure, and extensible architecture

---

## 📁 Project Structure

```plaintext
config/
  ├── config.yml       # LLM model config & max_tokens
  ├── rails.co         # Guardrails and moderation rules
  └── prompts.yml      # Prompt templates for moderation
main.py                # FastAPI app entry point
requirements.txt       # Python dependencies
.env                   # Environment variables (API key, etc.)
```

---

## ⚡ Quickstart

1. **Set up a virtual environment**  
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

2. **Install dependencies**  
   ```bash
   pip install -r requirements.txt
   ```

3. **Add your Groq API key to `.env`**  
   ```env
   GROQ_API_KEY=your-groq-api-key-here
   ```

4. **Start the FastAPI server**  
   ```bash
   uvicorn main:app --reload
   ```

5. **Test the chatbot**  
   ```bash
   curl -X POST -H "Content-Type: application/json" \
     -d "{\"message\": \"Tell me a fun fact about computers.\"}" \
     http://localhost:8000/chat
   ```

---

## 🧠 How It Works

- 🔒 User messages are filtered using NeMo Guardrails before reaching the LLM  
- 🔎 LLM responses are moderated before being returned  
- ⚠️ Unsafe messages are blocked with refusal prompts; safe messages receive normal responses

---

## 🛠️ Customization

- Modify `config/prompts.yml` to update moderation prompts  
- Change `max_tokens` in `config/config.yml` to set response limits

---

## ⚠️ Important

> **Keep your `.env` file secret. Never commit your API key.**

---

Happy building! 🚀
